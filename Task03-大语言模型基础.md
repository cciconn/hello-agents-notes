# Task03：大语言模型基础学习笔记

## 学习内容
1. 语言模型与Transformer架构
1.1 从N-gram到RNN
1.2 Tranformer架构解析
1.3 Decoder-Only架构
2. 与大语言模型交互
2.1 提示工程
   （1）模型采样参数
   （2）零样本、单样本与少样本提示
   （3）指令调优的影响
   （4）基础提示技巧
   （5）思维链
2.2 文本分词
   （1）为何需要分词
   （2）字节对编码算法解析
   （3）分词器对开发者的意义
2.3 调用开源大语言模型
2.4 模型的选择
3. 大语言模型的缩放法则与局限性
3.1 缩放法则
3.2 模型幻觉


## 习题
1.
- <img width="1896" height="1888" alt="image" src="https://github.com/user-attachments/assets/fd78c3b9-bca5-49a6-8931-220817f97c7e" />
- 含义：一个词的出现概率，只与其前面有限个（N-1）词有关，而不依赖于更久远的历史。
  局限性：上下文长度有限，模型只能捕捉到局部的（N-1阶）词序关系，无法建模长距离的依赖。数据稀疏性：如果一个词序列从未在语料库中出现，其概率估计就为 0，这显然是不合理的。虽然可以通过平滑技术缓解，但无法根除。泛化能力差：模型无法理解词与词之间的语义相似性。模型仅基于表面共现进行统计，无法理解词义、语法结构和逻辑关系。
- RNN/LSTM：通过循环网络传递隐藏状态，理论上可以记住更长的历史，克服了固定窗口问题；同时引入词嵌入，让语义相似的词向量接近，获得了泛化能力。
  - 优势：结构相对简单，顺序处理的特性与语言流天然契合，在它出现时显著提升了语言建模能力。
  Transformer：通过自注意力机制，直接计算序列中所有词之间的关系，一举实现了真正的全局上下文建模，彻底克服了长距离依赖问题。
  - 优势：并行计算效率极高，使训练超大模型成为可能；全局注意力对上下文的建模更精准、更强大。其核心优势如此显著，以至于已成为当今所有大语言模型的绝对基础架构。

2.
- 自注意力机制的核心思想是 让序列中的每个元素（如一个词）能够直接“看到”并权衡序列中所有其他元素的重要性，从而动态地构建其上下文感知的表示。
- RNN：必须按顺序处理数据。第t个时间步的计算，必须等待第t−1时间步完成后才能开始。这种时序上的递归依赖决定了计算必须一步一步进行，无法并行。
  Transformer：自注意力机制在计算某个位置的输出时，所需的输入是所有位置的词向量。这些词向量是同时作为输入的。
  位置编码作用：它为序列中每个位置的词向量添加一个唯一的、包含位置信息的向量。这样，即使两个词相同但位置不同，其初始输入向量也会不同。并行处理的能力源于自注意力的无递归结构；位置编码则弥补了这种结构对顺序信息的缺失，是并行化的必要前提。
- 完整Encoder-Decoder架构：最初为机器翻译设计，有编码器和解码器。Decoder-Only架构：只使用了解码器部分，并做了一些调整。
  Decoder-Only成为主流的原因：任务目标的统一：大语言模型的核心被定义为一个纯粹的自回归生成任务——根据上文预测下一个词。规模化优势：在大规模无监督文本（如整个互联网）上预训练时，Decoder-Only模型只需学习从海量文本中预测下一个词，这个目标简单且数据量无限。涌现的通用能力：当Decoder-Only模型的参数量和训练数据足够大时，仅仅通过“下一个词预测”目标，就能涌现出令人惊叹的理解、推理、编程等能力。无需复杂的多任务或架构设计。

3.
- 按词分词 (Word-based) ：直接用空格或标点符号将句子切分成单词。这种方法很直观，但会面临“词表爆炸”的问题。一个语言的词汇量是巨大的，如果每个词都作为一个独立的词元，词表会变得难以管理。更糟糕的是，模型将无法处理任何未在词表中出现过的词，例如 “DatawhaleAgent”。
  按字符分词 (Character-based) ：将文本切分成单个字符。这种方法词表很小（例如英文字母、数字和标点），不存在 OOV 问题。但它的缺点是，单个字符大多不具备独立的语义，模型需要花费更多的精力去学习如何将字符组合成有意义的词，导致学习效率低下。
- BPE算法提供了一种数据驱动的、自适应的折衷方案，它通过迭代合并，构建了一个由常见词和高频子词组成的、规模固定的词汇表。这解决了词汇表爆炸和OOV问题，使模型既能高效处理常见文本，又能通过组合的方式理解罕见词和新词，是大语言模型能够稳健处理开放世界文本的关键技术基础。

4.


5.
- 工作原理：RAG不依赖模型自身记忆所有知识，而是在回答问题时，像查资料一样，从外部权威知识库中实时检索相关信息，再根据这些信息生成答案，从源头减少“无中生有”的风险。
  适用场景：突破知识边界与时效性：能访问并基于最新、最专业的资料（如最新论文、公司文档）进行回答，特别适合知识密集且更新快的领域，如医疗咨询、法律分析、金融研究。提高事实准确性：答案源于可查证的资料，生成时可要求引用来源，方便事后验证。研究表明，引入RAG可将问答系统准确率显著提升。降低部署成本：无需为学习新知识而频繁重训或微调大模型，仅需更新外部知识库，成本更低。
- InEx框架：模仿人类“先思考后验证”的决策过程，让多个AI智能体分别担任生成、检查和修正的角色，通过内部协作减少错误。
  改进与优势：训练免费：无需重新训练模型，直接增强现有模型。自主迭代验证：通过多轮自省和交叉验证达成共识，可靠性高。多模态适用：能同时处理文本和图像，缓解多模态幻觉。


6.
- Claude 3.5 Sonnet 是一个极佳的平衡选择，它在200K上下文、推理能力、代码理解和成本控制上取得了很好的平衡。
- markdown
你是一位专业的学术研究助理,在学术论文查找、理解和总结方面拥有多年的经验和理解。你的任务是基于用户提供的学术论文，进行精确、客观的分析和总结。
请严格遵循以下步骤：
1.  **整体把握**：首先阅读摘要、引言和结论，用一段话概括论文的核心研究问题、主要结论和学术价值。
2.  **深度解析**：然后，针对[用户的具体问题，例如：方法论、图的数据、与某理论的关联]，提取相关章节的详细内容。
3.  **结构化输出**：请将你的分析按以下JSON格式输出：
{
  "core_contribution": "一段话总结",
  "key_methodology": "描述方法",
  "main_findings": ["发现1", "发现2"],
  "answers_to_specific_queries": "针对用户问题的回答",
  "cited_sections": ["引用的章节，如 'Section 3.2'", "引用的图表，如 'Figure 4'"]
}
请注意：所有信息必须严格基于论文原文，不得添加外部知识或个人臆测。如果原文对某个问题没有明确阐述，请明确标注“原文未提及”。
- 长上下文处理的工程化解决方案：检索增强生成：将论文切片（每块可包含1-2个段落或一个图表及其描述），并向量化存入数据库。当用户提问时，将问题向量化，从数据库中检索最相关的若干个文本块。仅将这些相关文本块（而非全文）与问题一起发送给模型生成答案。
- 输入层面：强制提供依据，结合RAG，设计系统工作流，使模型每一条关键陈述都必须引用自检索到的原文片段。处理层面：多智能体交叉验证。输出层面：置信度与不确定性量化，要求模型在输出中，对基于强证据（如直接数据、明确定义）和弱证据（如作者推论、间接暗示）的陈述进行区分标注。用户交互层面：提供反向链，系统界面不直接输出答案。当用户提问时，先展示从论文中检索到的相关原文片段，然后再展示基于这些片段的总结和答案。

